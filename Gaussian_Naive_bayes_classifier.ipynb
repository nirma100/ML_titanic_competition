{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":72528,"databundleVersionId":7946462,"sourceType":"competition"}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Titanic Survival Prediction\n## Introduction\nIn this project, we aim to predict the survival of passengers on the Titanic using various machine learning techniques. The dataset contains information about passengers such as their age, sex, class, and other details.","metadata":{}},{"cell_type":"markdown","source":"## 1. Load Libraries and Data\nWe start by importing the necessary libraries and loading the dataset.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, RobustScaler, PowerTransformer, QuantileTransformer, PolynomialFeatures\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, KFold, learning_curve\nfrom sklearn.feature_selection import mutual_info_classif, mutual_info_regression\nfrom imblearn.combine import SMOTETomek\nfrom scipy import stats\nfrom scipy.stats import skew, kurtosis, boxcox\nimport warnings\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_curve, auc\nimport string\n\n# For Age forecast\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import VotingRegressor\n\n# Suppress warnings\nwarnings.filterwarnings('ignore')\n\n\n# Set plot style for seaborn\nsns.set(style=\"whitegrid\")\n\nprint('Packages were loaded')\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-14T19:01:38.322593Z","iopub.execute_input":"2024-07-14T19:01:38.323166Z","iopub.status.idle":"2024-07-14T19:01:38.335555Z","shell.execute_reply.started":"2024-07-14T19:01:38.323127Z","shell.execute_reply":"2024-07-14T19:01:38.334086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Load Dataset\nWe will load the training and testing datasets.","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/2024-sce-ml-course/train_2022.csv')\ntest = pd.read_csv('/kaggle/input/2024-sce-ml-course/test_2022.csv')\n\nprint(train.info())\nprint(\"-\"*100)\nprint(test.info())\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Data Visualization and Analysis\nWe visualize the data to understand its distribution and identify any missing values. The function visualize_data_analysis is defined to generate different types of plots based on the data.","metadata":{}},{"cell_type":"code","source":"\ntrain[\"Survived_binary\"]=train[\"Survived\"].map({'T':1,'F': 0})\n\n\ndef visualize_data_analysis(df, df_name, q=[1, 2, 3, 4, 5], encoded_categorical_cols=['Survived', 'Sex', 'Embarked', 'Deck', 'Title', 'Title_Category', 'AgeGroup', 'Married']):\n    df_temp = df.copy()\n    \n    if 1 in q:\n        plt.figure(figsize=(10, 6))\n        sns.heatmap(df_temp.isnull(), cbar=False, cmap='viridis')\n        plt.title(f'Missing Values Distribution of {df_name}')\n        plt.show()\n\n    if 2 in q:\n        numeric_df = df_temp.select_dtypes(include=[np.number])\n        if not numeric_df.empty:\n            numeric_df.hist(bins=20, figsize=(20, 15), edgecolor='black')\n            plt.suptitle(f'Histograms of Numerical Features of {df_name}')\n            plt.show()\n\n    if 3 in q:\n        numeric_df = df_temp.select_dtypes(include=[np.number])\n        plt.figure(figsize=(25, 20))\n        sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm', fmt='.2f')\n        plt.title(f'Feature Correlation Matrix of {df_name}')\n        plt.show()\n    \n    if 4 in q:\n        numeric_df = df_temp.select_dtypes(include=[np.number])\n        if 'PassengerId' in numeric_df.columns:\n            numeric_df = numeric_df.drop(columns=['PassengerId'])\n        summary = numeric_df.describe().T\n        summary['median'] = numeric_df.median()\n        summary['skew'] = numeric_df.skew()\n        summary['kurtosis'] = numeric_df.kurtosis()\n\n        summary = summary[['mean', 'median', 'std', 'min', 'max', 'skew', 'kurtosis']]\n        summary.columns = ['Mean', 'Median', 'Std Dev', 'Min', 'Max', 'Skew', 'Kurtosis']\n\n        plt.figure(figsize=(12, 8))\n        sns.heatmap(summary, annot=True, cmap='coolwarm', cbar=False, fmt='.2f', linewidths=0.5)\n        plt.title(f'Summary Statistics of Numeric Features of {df_name}')\n        plt.show()\n        \n    if 5 in q:        \n        # Plot categorical features with pie and bar charts\n        for col in encoded_categorical_cols:\n            if col in df_temp.columns and df_temp[col].nunique() <= 7:  # Ensure the column exists in df_temp and has <= 7 unique values\n                fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n                # Use ax.pie directly to get wedges, texts, and autotexts\n                wedges, texts, autotexts = axes[0].pie(\n                    df_temp[col].value_counts(), autopct='%1.1f%%', colors=['#66b3ff', '#99ff99', '#ffcc99'], startangle=90\n                )\n                for text, autotext in zip(texts, autotexts):\n                    text.set_rotation(45)  # Rotate the labels\n                    autotext.set_rotation(45)\n                axes[0].set_title(f'{col} Distribution (Pie)')\n                axes[0].set_ylabel('')\n\n                sns.countplot(data=df_temp, x=col, ax=axes[1])\n                axes[1].set_title(f'{col} Distribution (Bar)')\n                plt.tight_layout()\n                plt.show()\n\n\n        \n# # Visualize training data characteristics\nvisualize_data_analysis(train, \"Train Data\",q=[1,2,3,4])\nvisualize_data_analysis(test, \"Test Data\", q=[1,4])","metadata":{"execution":{"iopub.status.busy":"2024-07-14T19:01:38.338331Z","iopub.execute_input":"2024-07-14T19:01:38.338839Z","iopub.status.idle":"2024-07-14T19:01:38.389432Z","shell.execute_reply.started":"2024-07-14T19:01:38.338803Z","shell.execute_reply":"2024-07-14T19:01:38.388282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Feature Engineering and Encoding\nWe create new features and encode categorical features to prepare the data for modeling.\n\n### New Features:\n1. Deck: Extracted from the cabin number and categorized into groups.\n1. Title: Extracted from the name and categorized into common and rare titles.\n1. FamilySize: Calculated as the sum of SibSp (siblings/spouses) and Parch (parents/children) plus one.\n1. IsAlone: A binary feature indicating whether the passenger is alone.\n1. FarePerPerson: Calculated as the fare divided by the family size.\n1. TicketPrefix and TicketNumber: Extracted from the ticket information.\n1. Title_Category: Grouped titles into common and royalty.\n1. Sex_binary: Converted sex into binary values.\n1. GroupSize: Count of people with the same ticket number.\n1. IsInGroup: A binary feature indicating whether the passenger is in a group.\n1. ParchSibSp: Product of Parch and SibSp.\n1. FareBin: Binned FarePerPerson into quantiles.\n1. CabinCount: Count of cabins assigned to a passenger.\n1. Family_Size_Grouped: Categorized family size into groups.\n\n","metadata":{}},{"cell_type":"code","source":"def extract_ticket_features(ticket):\n    ticket = ticket.replace('.', '').replace('/', '').split()\n    ticket_prefix = ticket[0] if not ticket[0].isdigit() else 'NoPrefix'\n    ticket_number = ticket[-1] if ticket[-1].isdigit() else 'NoNumber'\n    return ticket_prefix, ticket_number\n\ndef feature_engineering_encoding_missingData(df1, df2):\n    combine = [df1.copy(), df2.copy()]\n    for i in range(len(combine)):\n        df_temp = combine[i]\n        \n        # When I googled Stone, Mrs. George Nelson (Martha Evelyn), I found that she embarked from S (Southampton) with her maid Amelie Icard, in this page Martha Evelyn Stone: Titanic Survivor.\n        df_temp['Embarked'] = df_temp['Embarked'].fillna('S')\n\n        # Fill missing 'Cabin' with 'Unknown'\n        df_temp['Cabin'].fillna('Unknown', inplace=True)\n\n        # Create deck feature extracted from the cabin number and replace it to be align with Pclass\n        df_temp['Deck'] = df_temp['Cabin'].apply(lambda x: x[0])\n        df_temp['Deck'] = df_temp['Deck'].replace(['A', 'B', 'C'], 'ABC')\n        df_temp['Deck'] = df_temp['Deck'].replace(['D', 'E'], 'DE')\n        df_temp['Deck'] = df_temp['Deck'].replace(['F', 'G'], 'FG')\n        \n\n        # Create new features\n        df_temp['Title'] = df_temp['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())\n        df_temp['Title'] = df_temp['Title'].replace(['Lady', 'Countess', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Master', 'Jonkheer', 'Dona'], 'Rare')\n        df_temp['Title'] = df_temp['Title'].replace(['Mlle', 'Ms'], 'Miss')\n        df_temp['Title'] = df_temp['Title'].replace('Mme', 'Mrs')\n        df_temp['Married'] = df_temp['Title'].apply(lambda x: 1 if x == 'Mrs' else 0)\n        df_temp['Title'] = df_temp['Title'].apply(lambda x: 'Rare' if x not in ['Mr', 'Mrs', 'Miss'] else x)\n\n        df_temp['FamilySize'] = df_temp['SibSp'] + df_temp['Parch'] + 1\n        df_temp['IsAlone'] = df_temp['FamilySize'].apply(lambda x: 1 if x == 1 else 0)\n        \n        # Median of a Fare satisying condition([3][0][0] -- 3=Pclass,0=Parch,SibSp) based on the only missing sample\n        # Filling the missing value in Fare with the median Fare of 3rd class alone passenger\n        med_fare = df_temp.groupby(['Pclass', 'Parch', 'SibSp'])['Fare'].median()[3][0][0]\n        df_temp['Fare'] = df_temp['Fare'].fillna(med_fare)\n        \n        df_temp['FarePerPerson'] = df_temp['Fare'] / df_temp['FamilySize']\n        df_temp = df_temp.drop(columns=['Fare'])\n\n        df_temp[['TicketPrefix', 'TicketNumber']] = df_temp['Ticket'].apply(lambda x: pd.Series(extract_ticket_features(x)))\n\n        title_mapping = {'Mr': 'Common', 'Mrs': 'Common', 'Miss': 'Common', 'Master': 'Common', 'Rare': 'Royalty'}\n        df_temp['Title_Category'] = df_temp['Title'].map(title_mapping)\n\n        if 'Survived' in df_temp.columns:\n            df_temp['Survived_binary'] = df_temp['Survived'].map({'T': 1, 'F': 0})\n\n        df_temp['Sex_binary'] = df_temp['Sex'].map({'male': 0, 'female': 1})\n        df_temp['GroupSize'] = df_temp.groupby('Ticket')['Ticket'].transform('count')\n        df_temp['IsInGroup'] = df_temp['GroupSize'].apply(lambda x: 1 if x > 1 else 0)\n\n        # Apply one-hot encoding\n        one_hot_encoded = pd.get_dummies(df_temp[['Title','Married']], drop_first=True)\n        one_hot_encoded = pd.get_dummies(df_temp[['Embarked','Sex_binary','Deck',]], drop_first=False)\n\n        one_hot_encoded = one_hot_encoded.astype(int)\n        df_temp = df_temp.drop(columns=['Sex_binary', 'IsAlone' ])\n        df_temp = pd.concat([df_temp, one_hot_encoded], axis=1)\n\n        df_temp['ParchSibSp'] = df_temp['Parch'] * df_temp['SibSp']\n        \n        df_temp['FareBin'] = pd.qcut(df_temp['FarePerPerson'], 13, labels=False)\n        \n        df_temp['CabinCount'] = df_temp['Cabin'].apply(lambda x: len(x.split()) if pd.notna(x) else 0)\n        \n        # Mapping Family Size\n        family_map = {1: 'Alone', 2: 'Small', 3: 'Small', 4: 'Small', 5: 'Medium', 6: 'Medium', 7: 'Large', 8: 'Large', 9: 'Large', 10: 'Large', 11: 'Large'}\n        df_temp['Family_Size_Grouped'] = df_temp['FamilySize'].map(family_map)\n        \n        combine[i] = df_temp\n\n    return combine[0], combine[1]\n\ntrain_encoded, test_encoded = feature_engineering_encoding_missingData(train, test)\n\nvisualize_data_analysis(train_encoded, \"Train Data\",q=[4])","metadata":{"execution":{"iopub.status.busy":"2024-07-14T19:01:38.391062Z","iopub.execute_input":"2024-07-14T19:01:38.391419Z","iopub.status.idle":"2024-07-14T19:01:39.876648Z","shell.execute_reply.started":"2024-07-14T19:01:38.391388Z","shell.execute_reply":"2024-07-14T19:01:39.875363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Explore Feature Distributions Segmented by Survival\nVisualize how different features are distributed with respect to the survival variable.","metadata":{}},{"cell_type":"code","source":"cat_features = ['Embarked', 'Parch', 'Pclass', 'Sex', 'SibSp', 'Deck','Title','IsInGroup','Married','GroupSize','FareBin','Family_Size_Grouped']\n\nfig, axs = plt.subplots(ncols=2, nrows=3, figsize=(20, 20))\nplt.subplots_adjust(right=1.5, top=1.25)\n\nfor i, feature in enumerate(cat_features, 1):    \n    plt.subplot(4, 3, i)\n    sns.countplot(x=feature, hue='Survived', data=train_encoded)\n    \n    plt.xlabel('{}'.format(feature), size=20, labelpad=15)\n    plt.ylabel('Passenger Count', size=20, labelpad=15)    \n    plt.tick_params(axis='x', labelsize=20)\n    plt.tick_params(axis='y', labelsize=20)\n    \n    plt.legend(['Not Survived', 'Survived'], loc='upper center', prop={'size': 18})\n    plt.title('Count of Survival in {} Feature'.format(feature), size=20, y=1.05)\n    fig.subplots_adjust(hspace=0.4, wspace=0.3)  # Adjust the values as needed\n\nplt.show()\n\nle = LabelEncoder()\ncategorical_cols = ['Embarked', 'Deck', 'Title', 'TicketPrefix', 'Title_Category','Family_Size_Grouped','TicketNumber']\ncombine = [train_encoded, test_encoded]\nfor df_temp in combine:\n    for col in categorical_cols:\n            df_temp[col] = le.fit_transform(df_temp[col].astype(str))","metadata":{"execution":{"iopub.status.busy":"2024-07-14T19:01:39.879415Z","iopub.execute_input":"2024-07-14T19:01:39.879860Z","iopub.status.idle":"2024-07-14T19:01:42.673159Z","shell.execute_reply.started":"2024-07-14T19:01:39.879822Z","shell.execute_reply":"2024-07-14T19:01:42.671950Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Forecast Missing Ages\nWe will forecast missing ages using machine learning models.\n\n### New Age Features:\n1. AgeGroupNum: Binned age into numerical categories.\n1. AgeGroup: Binned age into categorical groups such as Baby, Child, etc.\n1. age_sex_string: Combined age group and sex into a single string.\n1. age_sex_encoded: Encoded age_sex_string into numerical values.\n1. IsChild: A binary feature indicating whether the passenger is a child (age < 18).\n\n### Methods Used:\n1. Mutual Information (MI): Used to select the most important features for predicting age.\n1. Skewness and Kurtosis: Used to identify and transform skewed features.","metadata":{}},{"cell_type":"code","source":"def separate_missing_ages(df):\n    df_missing_age = df[df['Age'].isnull()]\n    df_non_missing_age = df[df['Age'].notnull()]\n    return df_missing_age, df_non_missing_age\n\ndef select_features(X, y, num_features, plot=True):\n    # Ensure all features are numeric\n    X_numeric = X.select_dtypes(include=[np.number])\n    mi = mutual_info_regression(X_numeric, y)\n    mi_series = pd.Series(mi, index=X_numeric.columns)\n    selected_features = mi_series.sort_values(ascending=False).head(num_features).index.tolist()\n\n    if plot:\n        plt.figure(figsize=(10, 6))\n        mi_series.sort_values(ascending=False).plot(kind='bar')\n        plt.title('Mutual Information of Features with Age')\n        plt.xlabel('Features')\n        plt.ylabel('Mutual Information Score')\n        plt.show()\n\n    return sorted(selected_features)\n\n# Define the function to train age predictors\ndef train_age_predictors(X, y):\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n\n    param_grid_dt = {\n        'max_depth': [None, 10, 20, 30],\n        'min_samples_split': [2, 5, 10],\n        'min_samples_leaf': [1, 2, 4],\n        'max_features': ['auto', 'sqrt', 'log2']\n    }\n\n    param_grid_knn = {\n        'n_neighbors': [3, 5, 7, 9, 11],\n        'weights': ['uniform', 'distance'],\n        'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n    }\n\n    models = {\n        'Decision Tree': (DecisionTreeRegressor(), param_grid_dt),\n        'K-Nearest Neighbors': (KNeighborsRegressor(), param_grid_knn)\n    }\n\n    best_models = {}\n    for name in models:\n        model, param_grid = models[name]\n        grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error')\n        grid_search.fit(X_scaled, y)\n        best_models[name] = grid_search.best_estimator_\n        print(f\"Best parameters for {name}: {grid_search.best_params_}\")\n        print(f\"Best neg MSE score for {name}: {grid_search.best_score_}\")\n\n    voting_regressor = VotingRegressor(estimators=[\n        ('dt', best_models['Decision Tree']),\n        ('knn', best_models['K-Nearest Neighbors'])\n    ])\n    voting_regressor.fit(X_scaled, y)\n\n    return best_models, voting_regressor, scaler\n\n# Function to predict missing ages\ndef predict_missing_ages(voting_regressor, scaler, df_missing_age, features):\n    X_missing = df_missing_age[features].sort_index(axis=1)\n    X_missing_scaled = scaler.transform(X_missing)\n    df_missing_age['Age'] = voting_regressor.predict(X_missing_scaled)\n    return df_missing_age\n\ndef get_skewed_features(df, threshold=0.5, high_skew_threshold=1.0):\n    df_copy = df.copy()\n    numerical_features = df_copy.select_dtypes(include=[np.number]).columns.tolist()\n    excluded_features = ['Sex_binary', 'IsChild', 'Married', 'Title_Category', 'PassengerId']\n    numerical_features = [feature for feature in numerical_features if feature not in excluded_features]\n    \n    skew_df = df_copy[numerical_features].apply(lambda x: x.skew())\n    kurtosis_df = df_copy[numerical_features].apply(lambda x: x.kurtosis())\n    skew_df = skew_df.sort_values(ascending=False)\n    skew_df = skew_df.reset_index()\n    skew_df.columns = ['Feature', 'SkewFactor']\n    kurtosis_df = kurtosis_df.reset_index()\n    kurtosis_df.columns = ['Feature', 'Kurtosis']\n    skew_kurtosis_df = pd.merge(skew_df, kurtosis_df, on='Feature')\n    highly_skewed_features = list(skew_kurtosis_df[skew_kurtosis_df['SkewFactor'] > high_skew_threshold]['Feature'])\n    moderately_skewed_features = list(skew_kurtosis_df[(skew_kurtosis_df['SkewFactor'] > threshold) & (skew_kurtosis_df['SkewFactor'] <= high_skew_threshold)]['Feature'])\n    non_skewed_features = list(set(numerical_features) - set(moderately_skewed_features) - set(highly_skewed_features))\n    return highly_skewed_features, moderately_skewed_features, non_skewed_features, skew_kurtosis_df\n\ndef transform_highly_skewed_features(df, highly_skewed_features):\n    df_copy = df.copy()\n    for col in highly_skewed_features:\n        try:\n            transformed_data, _ = stats.boxcox(df_copy[col] + 1)  # Ensure positive values\n            df_copy[col] = transformed_data\n        except Exception:\n            df_copy[col] = np.log1p(df_copy[col])\n    return df_copy\n\ndef transform_moderately_skewed_features(df, moderately_skewed_features):\n    df_copy = df.copy()\n    for col in moderately_skewed_features:\n        df_copy[col] = np.log1p(df_copy[col])\n    return df_copy\n\ndef transform_non_skewed_features(df, non_skewed_features):\n    df_copy = df.copy()\n    scaler = StandardScaler()\n    df_copy[non_skewed_features] = scaler.fit_transform(df_copy[non_skewed_features])\n    return df_copy\n\ndef skew_groups(df_train, df_test):\n    combine = [df_train.copy(), df_test.copy()]\n    for i in range(len(combine)):\n        df_temp = combine[i]\n        highly_skewed_features, moderately_skewed_features, non_skewed_features, skew_kurtosis_df = get_skewed_features(df_temp, threshold=0.5, high_skew_threshold=1.0)\n        df_temp = transform_highly_skewed_features(df_temp, highly_skewed_features)\n        df_temp = transform_moderately_skewed_features(df_temp, moderately_skewed_features)\n        df_temp = transform_non_skewed_features(df_temp, non_skewed_features)\n        combine[i] = df_temp\n    return combine[0], combine[1]\n\ndef visualize_distributions(df, highly_skewed_features, moderately_skewed_features, skew_kurtosis_df, threshold=0.1):\n    df_copy = df.copy()\n    significant_features = highly_skewed_features + moderately_skewed_features\n    \n    num_features = len(significant_features)\n    if num_features == 0:\n        print(\"No significant changes in skewness detected.\")\n        return\n    \n    fig, axes = plt.subplots(nrows=num_features, ncols=2, figsize=(14, 6 * num_features))\n    \n    for i, col in enumerate(significant_features):\n        original_skewness = df_copy[col].skew()\n        original_kurtosis = skew_kurtosis_df[skew_kurtosis_df['Feature'] == col]['Kurtosis'].values[0]\n        sns.histplot(df_copy[col].dropna(), kde=True, ax=axes[i, 0], color='blue', bins=30)\n        axes[i, 0].set_title(f'Before: {col} (Skewness: {original_skewness:.2f}, Kurtosis: {original_kurtosis:.2f})', fontsize=14)\n        axes[i, 0].set_xlabel('')\n        axes[i, 0].set_ylabel('')\n    \n    # Apply the transformations\n    df_copy = transform_highly_skewed_features(df_copy, highly_skewed_features)\n    df_copy = transform_moderately_skewed_features(df_copy, moderately_skewed_features)\n    df_copy = transform_non_skewed_features(df_copy, list(set(df_copy.select_dtypes(include=[np.number]).columns) - set(significant_features)))\n    \n    for i, col in enumerate(highly_skewed_features):\n        transformed_skewness = df_copy[col].skew()\n        transformed_kurtosis = df_copy[col].kurtosis()\n        sns.histplot(df_copy[col].dropna(), kde=True, ax=axes[i, 1], color='green', bins=30)\n        axes[i, 1].set_title(f'After (Box-Cox): {col} (Skewness: {transformed_skewness:.2f}, Kurtosis: {transformed_kurtosis:.2f})', fontsize=14)\n        axes[i, 1].set_xlabel('')\n        axes[i, 1].set_ylabel('')\n    \n    for i, col in enumerate(moderately_skewed_features, len(highly_skewed_features)):\n        transformed_skewness = df_copy[col].skew()\n        transformed_kurtosis = df_copy[col].kurtosis()\n        sns.histplot(df_copy[col].dropna(), kde=True, ax=axes[i, 1], color='green', bins=30)\n        axes[i, 1].set_title(f'After (Log): {col} (Skewness: {transformed_skewness:.2f}, Kurtosis: {transformed_kurtosis:.2f})', fontsize=14)\n        axes[i, 1].set_xlabel('')\n        axes[i, 1].set_ylabel('')\n    \n    plt.tight_layout()\n    plt.show()\n    return df_copy\n\n\n\n# Separate rows with missing and non-missing ages for training data\ndf_missing_age_train, df_non_missing_age_train = train_encoded[train_encoded['Age'].isnull()], train_encoded[train_encoded['Age'].notnull()]\ndf_missing_age_test, df_non_missing_age_test = test_encoded[test_encoded['Age'].isnull()], test_encoded[test_encoded['Age'].notnull()]\n\n# Prepare training data\nX_train = df_non_missing_age_train.drop(columns=['Age'])\ny_train = df_non_missing_age_train['Age']\n\n# Select best features using MI\nselected_features = select_features(X_train, y_train, num_features=7)\nX_train_selected = X_train[selected_features]\n\n# Train the models\n_, voting_regressor, scaler = train_age_predictors(X_train_selected, y_train)\n\n# Ensure the selected features are present in both train and test datasets\ncommon_features = list(set(selected_features) & set(df_missing_age_test.columns))\n\n# Scale the data\nX_train_selected_scaled = scaler.fit_transform(X_train_selected)\n\n# Train the voting regressor on scaled data\nvoting_regressor.fit(X_train_selected_scaled, y_train)\n\n# Process skewness for missing age data\ndf_missing_age_train, df_missing_age_test = skew_groups(df_missing_age_train, df_missing_age_test)\n\n\n# Predict missing ages for training and test data\ndf_missing_age_train = predict_missing_ages(voting_regressor, scaler, df_missing_age_train, common_features)\ndf_missing_age_test = predict_missing_ages(voting_regressor, scaler, df_missing_age_test, common_features)\n\n# Combine the non-missing and newly predicted ages for training and test data\ntrain_encoded['Age'] = pd.concat([df_non_missing_age_train['Age'], df_missing_age_train['Age']])\ntest_encoded['Age'] = pd.concat([df_non_missing_age_test['Age'], df_missing_age_test['Age']])\n\ndef age_related_features(df):\n    age_bins = [0, 2, 12, 18, 25, 35, 60, np.inf]\n    age_labels = [1, 2, 3, 4, 5, 6, 7]\n    df['AgeGroupNum'] = pd.cut(df['Age'], bins=age_bins, labels=age_labels).astype(int)\n\n    age_labels = ['Baby', 'Child', 'Teenager', 'Young Adult', 'Adult', 'Middle Aged', 'Senior']\n    df['AgeGroup'] = pd.cut(df['Age'], bins=age_bins, labels=age_labels)\n    df['age_sex_string'] = (df['AgeGroup'].str.lower() + ' - ' + df['Sex'].str.lower())\n\n    label_encoder = LabelEncoder()\n    df['age_sex_encoded'] = label_encoder.fit_transform(df['age_sex_string'])\n    df['IsChild'] = df['Age'].apply(lambda x: 1 if x < 18 else 0)\n    \n\n\n\nage_related_features(train_encoded)\nage_related_features(test_encoded)\n\nvisualize_data_analysis(train_encoded, \"Train Data\", [4, 5],encoded_categorical_cols=['Survived','AgeGroup','IsChild','age_sex_string'])\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-14T19:01:42.674620Z","iopub.execute_input":"2024-07-14T19:01:42.674995Z","iopub.status.idle":"2024-07-14T19:01:48.351152Z","shell.execute_reply.started":"2024-07-14T19:01:42.674963Z","shell.execute_reply":"2024-07-14T19:01:48.350076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7. Final Standardization\nStandardize and transform the final features for modeling.\n\n### Transformations:\n1. Age: Standard scaling to normalize the distribution.\n1. FarePerPerson: Log transformation to handle high skewness.\n1. TicketNumber: Quantile transformation to handle kurtosis.\n\n* drop sex binary since sex_age is better scored by MI","metadata":{}},{"cell_type":"code","source":"\n# Drop the unnecessary columns\ntrain_encoded = train_encoded.drop(columns=['Sex_binary', 'Married'])\ntest_encoded = test_encoded.drop(columns=['Sex_binary', 'Married'])\n\n\ndef plot_distributions(original_df, transformed_df, features, title_prefix):\n    for feature in features:\n        fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n        # Original data\n        sns.histplot(original_df[feature].dropna(), kde=True, ax=axes[0])\n        axes[0].set_title(f'{title_prefix} - Original {feature}')\n        axes[0].text(1.02, 0.6, f\"Mean: {original_df[feature].mean():.2f}\\n\"\n                                f\"Median: {original_df[feature].median():.2f}\\n\"\n                                f\"Std: {original_df[feature].std():.2f}\\n\"\n                                f\"Skew: {skew(original_df[feature].dropna()):.2f}\\n\"\n                                f\"Kurtosis: {kurtosis(original_df[feature].dropna()):.2f}\",\n                                transform=axes[0].transAxes)\n\n        # Transformed data\n        sns.histplot(transformed_df[feature].dropna(), kde=True, ax=axes[1])\n        axes[1].set_title(f'{title_prefix} - Transformed {feature}')\n        axes[1].text(1.02, 0.6, f\"Mean: {transformed_df[feature].mean():.2f}\\n\"\n                                f\"Median: {transformed_df[feature].median():.2f}\\n\"\n                                f\"Std: {transformed_df[feature].std():.2f}\\n\"\n                                f\"Skew: {skew(transformed_df[feature].dropna()):.2f}\\n\"\n                                f\"Kurtosis: {kurtosis(transformed_df[feature].dropna()):.2f}\",\n                                transform=axes[1].transAxes)\n\n        plt.tight_layout()\n        plt.show()\n\ndef transform_and_plot(train_df, test_df, verbose=1):\n    numerical_features = ['Age', 'FarePerPerson', 'TicketNumber'] #'CabinCount','SibSp', 'Parch', 'FamilySize',  'GroupSize', 'ParchSibSp'\n\n    # Copy original data for plotting\n    original_train = train_df[numerical_features].copy()\n\n    # Initialize transformers\n    transformers = {\n        'FarePerPerson': QuantileTransformer(output_distribution='normal'),\n        'TicketNumber': QuantileTransformer(output_distribution='normal'),\n        'Age': QuantileTransformer(output_distribution='normal'),\n#         'SibSp': QuantileTransformer(output_distribution='normal'),\n#         'Parch': QuantileTransformer(output_distribution='normal'),\n#         'FamilySize': QuantileTransformer(output_distribution='normal'),\n#         'TicketPrefix': QuantileTransformer(output_distribution='normal'),\n#         'GroupSize': QuantileTransformer(output_distribution='normal'),\n#         'ParchSibSp': QuantileTransformer(output_distribution='normal'),\n#         'CabinCount': QuantileTransformer(output_distribution='normal'),\n#         'age_sex_encoded': QuantileTransformer(output_distribution='normal')\n    }\n     \n\n    # Apply the transformations to train data\n    for feature, transformer in transformers.items():\n        train_df[feature] = transformer.fit_transform(train_df[[feature]])\n\n    # Apply the same transformations to test data\n    for feature, transformer in transformers.items():\n        test_df[feature] = transformer.transform(test_df[[feature]])\n\n    if verbose:\n        plot_distributions(original_train, train_df, numerical_features, 'Train Data')\n\n    return train_df, test_df\n\n# Apply transformations and plot\ntrain_encoded, test_encoded = transform_and_plot(train_encoded, test_encoded, verbose=1)\nvisualize_data_analysis(train_encoded, \"Train Data\", [3,4])\n","metadata":{"execution":{"iopub.status.busy":"2024-07-14T19:01:48.352947Z","iopub.execute_input":"2024-07-14T19:01:48.353641Z","iopub.status.idle":"2024-07-14T19:01:56.717969Z","shell.execute_reply.started":"2024-07-14T19:01:48.353599Z","shell.execute_reply":"2024-07-14T19:01:56.716651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8. Adding Survival Rate Features\nWe add survival rate features based on family and ticket information.","metadata":{}},{"cell_type":"code","source":"\n# Function to extract family surname\ndef extract_surname(name):\n    if '(' in name:\n        name_no_bracket = name.split('(')[0]\n    else:\n        name_no_bracket = name\n    family = name_no_bracket.split(',')[0].strip()\n    return ''.join([c for c in family if c not in string.punctuation])\n\n# Function to add family and ticket survival rates\ndef add_survival_rates(combine):\n    for df in combine:\n        df['Family'] = df['Name'].apply(extract_surname)\n        df['Ticket_Frequency'] = df.groupby('Ticket')['Ticket'].transform('count')\n    \n    df_train = combine[0]\n    df_test = combine[1]\n\n    non_unique_families = set(df_train['Family']).intersection(set(df_test['Family']))\n    non_unique_tickets = set(df_train['Ticket']).intersection(set(df_test['Ticket']))\n    \n    family_rates = df_train.groupby('Family')['Survived_binary'].median()\n    ticket_rates = df_train.groupby('Ticket')['Survived_binary'].median()\n    \n    mean_survival_rate = df_train['Survived_binary'].mean()\n    \n    for df in combine:\n        df['Family_Survival_Rate'] = df['Family'].apply(lambda x: family_rates[x] if x in non_unique_families else mean_survival_rate)\n        df['Ticket_Survival_Rate'] = df['Ticket'].apply(lambda x: ticket_rates[x] if x in non_unique_tickets else mean_survival_rate)\n        df['Family_Survival_Rate_NA'] = df['Family'].apply(lambda x: 1 if x in non_unique_families else 0)\n        df['Ticket_Survival_Rate_NA'] = df['Ticket'].apply(lambda x: 1 if x in non_unique_tickets else 0)\n        df['Survival_Rate'] = (df['Family_Survival_Rate'] + df['Ticket_Survival_Rate']) / 2\n        df['Survival_Rate_NA'] = (df['Family_Survival_Rate_NA'] + df['Ticket_Survival_Rate_NA']) / 2\n    \n    columns_to_drop = ['Family_Survival_Rate', 'Ticket_Survival_Rate', \n                       'Family_Survival_Rate_NA', 'Ticket_Survival_Rate_NA']\n    \n    for df in combine:\n        df.drop(columns=columns_to_drop, errors='ignore', inplace=True)\n    \n    return combine\n\n# Combine train and test datasets\ncombine = [train_encoded, test_encoded]\n\n# Apply the function to add and drop survival rates\ncombine = add_survival_rates(combine)\n\n# Unpack the combine list back to train_encoded and test_encoded\ntrain_encoded, test_encoded = combine\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-14T19:01:56.719968Z","iopub.execute_input":"2024-07-14T19:01:56.720397Z","iopub.status.idle":"2024-07-14T19:01:56.773583Z","shell.execute_reply.started":"2024-07-14T19:01:56.720359Z","shell.execute_reply":"2024-07-14T19:01:56.772495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 9. Prepare Data and Apply SMOTE\nWe prepare the data for modeling and apply SMOTE to handle class imbalance.\n\n### Feature Selection:\n**Mutual Information (MI):** Used to select the most important features for survival prediction.","metadata":{}},{"cell_type":"code","source":"\n\n# # Function to select features based on mutual information for the survival prediction task\ndef select_survival_features(X, y, num_features, plot=True):\n    # Select only numeric features\n    X_numeric = X.select_dtypes(include=[np.number])\n    \n    # Calculate mutual information scores\n    mi = mutual_info_classif(X_numeric, y)\n    mi_series = pd.Series(mi, index=X_numeric.columns)\n    sorted_mi_series = mi_series.sort_values(ascending=False)\n    \n    # Select top features\n    selected_features = sorted_mi_series.head(num_features).index.tolist()\n\n    # Plot mutual information scores\n    if plot:\n        plt.figure(figsize=(10, 6))\n        sorted_mi_series.plot(kind='bar')\n        plt.title('Mutual Information of Features with Survival')\n        plt.xlabel('Features')\n        plt.ylabel('Mutual Information Score')\n        plt.show()\n    \n    # Evaluate model performance with different numbers of top features\n    scores = []\n    num_features_range = range(1, 15)\n    for num in num_features_range:\n        selected_features = sorted_mi_series.index[:num]\n        X_selected = X_numeric[selected_features]\n        gnb = GaussianNB()        \n        cv_scores = cross_val_score(gnb, X_selected, y, cv=5, scoring='accuracy')\n        scores.append(cv_scores.mean())\n    \n    # Plot the model performance\n    plt.figure(figsize=(10, 6))\n    plt.plot(num_features_range, scores, marker='o')\n    plt.title('Model Performance vs. Number of Features')\n    plt.xlabel('Number of Features')\n    plt.ylabel('Accuracy')\n    plt.grid(True)\n    plt.show()\n\n    # Find the optimal number of features\n    optimal_num_features = num_features_range[scores.index(max(scores))]\n    print(f\"Optimal number of features: {optimal_num_features}\")\n    return sorted_mi_series.index[:optimal_num_features].tolist()\n\n\ndef train_gnb(X, y):\n    param_grid = {'var_smoothing': np.logspace(0, -9, num=100)}\n    grid_search = GridSearchCV(GaussianNB(), param_grid, cv=5, scoring='accuracy')\n    grid_search.fit(X, y)\n    \n    print(f\"Best parameters: {grid_search.best_params_}\")\n    print(f\"Best accuracy: {grid_search.best_score_}\")\n    return grid_search.best_estimator_\n\n# Apply SMOTE + TOMEK\ndef apply_smote_tomek(X, y, sampling_strategy=0.75):\n    smote_tomek = SMOTETomek(sampling_strategy=sampling_strategy, random_state=42)\n    X_res, y_res = smote_tomek.fit_resample(X, y)\n    return X_res, y_res\n\n\n# Function to evaluate the model using confusion matrix and classification report\ndef evaluate_model(model, X, y):\n    y_pred = model.predict(X)\n    cm = confusion_matrix(y, y_pred)\n    cr = classification_report(y, y_pred)\n    accuracy = accuracy_score(y, y_pred)\n    fpr, tpr, _ = roc_curve(y, model.predict_proba(X)[:, 1])\n    roc_auc = auc(fpr, tpr)\n    \n    print(f\"Confusion Matrix:\\n{cm}\")\n    print(f\"Classification Report:\\n{cr}\")\n    print(f\"Accuracy: {accuracy}\")\n    \n    plt.figure(figsize=(10, 6))\n    plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f})')\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend(loc='lower right')\n    plt.show()\n\n    \ndef predict_and_prepare_submission(model, X_test, test, filename='submission_dt.csv'):\n    predictions = model.predict(X_test)\n    submission = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': predictions})\n    submission['Survived'] = submission['Survived'].map({1: 'T', 0: 'F'})\n    submission.to_csv(filename, index=False)\n    print(f\"Submission file '{filename}' created.\")\n\n\n# Separate features and target variable from training data\nX_train = train_encoded.drop(columns=['Survived', 'Survived_binary'])\n#high correlated feature to more Important features\nX_train = X_train.drop(columns=['FareBin','GroupSize','Pclass','Deck_U','FamilySize','Deck'])\ny_train = pd.DataFrame(train_encoded['Survived_binary'], columns=['Survived_binary'])\n\n\n# Select best features for survival prediction using MI\nselected_survival_features = select_survival_features(X_train, y_train, num_features=5)\nX_train_selected = X_train[selected_survival_features]\n\n# Apply SMOTE to handle class imbalance + TOMEK to toss noisy data\nX_train_resampled, y_train_resampled = apply_smote_tomek(X_train_selected, y_train)\n# X_train_resampled, y_train_resampled = X_train_selected, y_train\n\n\n# Train and tune the Decision Tree classifier\ngnb = train_gnb(X_train_resampled, y_train_resampled)\n\n\n# Prepare test data for prediction\nX_test = test_encoded[selected_survival_features]\n\n# Predict and prepare the submission file\npredict_and_prepare_submission(gnb, X_test, test)\n\n\n\n# Visualize the class distribution\nvisualize_data_analysis(X_train_resampled, \"Resampled X_Train Data\", [3])\nvisualize_data_analysis(y_train_resampled, \"Resampled y_Train Data\", [5], ['Survived_binary'])\nX_train_resampled.info()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T19:01:56.777043Z","iopub.execute_input":"2024-07-14T19:01:56.777518Z","iopub.status.idle":"2024-07-14T19:02:04.524178Z","shell.execute_reply.started":"2024-07-14T19:01:56.777475Z","shell.execute_reply":"2024-07-14T19:02:04.523054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 10. Model Evaluation and Cross-Validation\nWe evaluate the model using K-fold cross-validation and learning curves.","metadata":{}},{"cell_type":"code","source":"def kfold_cross_validation(model, X, y, k_values=range(4,9)):\n    best_k = None\n    best_score = -np.inf\n    best_cross_val_scores = None\n\n    for k in k_values:\n        kf = KFold(n_splits=k, shuffle=True, random_state=42)\n        cross_val_scores = cross_val_score(model, X, y, cv=kf, scoring='accuracy')\n        mean_score = np.mean(cross_val_scores).round(2)\n        std_score = np.std(cross_val_scores).round(2)\n\n        print(f\"Cross-validation scores for k={k}: {cross_val_scores}\\n\")\n        print(f\"Mean cross-validation score for k={k}: {mean_score}\\n\")\n        print(f\"Standard deviation of cross-validation scores for k={k}: {std_score}\")\n\n\n        if mean_score > best_score:\n            best_score = mean_score\n            best_k = k\n            best_std = std_score\n            best_cross_val_scores = cross_val_scores\n\n    print(f\"Best k: {best_k} \\n\")\n    print(f\"Best mean cross-validation score: {best_score}\\n\")\n    print(f\"Standard deviation of the best cross-validation scores: {best_std} \\n\\n\\n\")\n\n    return best_k, best_cross_val_scores\n\n\n\ndef plot_learning_curve(estimator, title, X, y, cv=None, n_jobs=None, train_sizes=np.linspace(0.1, 1.0, 10)):\n    plt.figure(figsize=(10, 6))\n    plt.title(title)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n\n    train_sizes, train_scores, valid_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring='accuracy')\n\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    valid_scores_mean = np.mean(valid_scores, axis=1)\n    valid_scores_std = np.std(valid_scores, axis=1)\n\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n    plt.fill_between(train_sizes, valid_scores_mean - valid_scores_std,\n                     valid_scores_mean + valid_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n    plt.plot(train_sizes, valid_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\n# Evaluate the model by ROC and conf matrix\nevaluate_model(gnb, X_train_resampled, y_train_resampled)\n\n# Plot the learning curve\nbest_k,_= kfold_cross_validation( gnb, X_train_resampled, y_train_resampled)\nplot_learning_curve(gnb, f\"Learning Curve (Decision Tree) with k={best_k}\", X_train_resampled, y_train_resampled, cv=best_k)\nplt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-14T19:02:04.526298Z","iopub.execute_input":"2024-07-14T19:02:04.526746Z","iopub.status.idle":"2024-07-14T19:02:06.112373Z","shell.execute_reply.started":"2024-07-14T19:02:04.526705Z","shell.execute_reply":"2024-07-14T19:02:06.111083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion\n\nThis kernel demonstrates the steps to preprocess the data, engineer new features, handle missing values, and apply machine learning techniques to predict the survival of Titanic passengers. The Gaussian Naive Bayes model was used, and the following evaluations were made:\n\n### Model Evaluation\n\n1. **Confusion Matrix**:\n   ```\n   [[455  50]\n    [121 246]]\n   ```\n   - True Negatives (TN): 455\n   - False Positives (FP): 50\n   - False Negatives (FN): 121\n   - True Positives (TP): 246\n\n2. **Classification Report**:\n   ```\n                 precision    recall  f1-score   support\n\n              0       0.79      0.90      0.84       505\n              1       0.83      0.67      0.74       367\n\n       accuracy                           0.80       872\n      macro avg       0.81      0.79      0.79       872\n   weighted avg       0.81      0.80      0.80       872\n   ```\n   - Overall accuracy: 80.39%\n   - Precision, recall, and f1-score for each class indicate good performance, with a slight drop in recall for the positive class (survived).\n\n### Cross-Validation\n\n1. **Cross-Validation Scores**:\n   ```\n     Best k: 4 \n     Best mean cross-validation score: 0.8\n     Standard deviation of the best cross-validation scores: 0.02 \n   ```\n   \n\n2. **Optimal Number of Folds (k)**:\n   - The best cross-validation score was achieved with `k=4`, yielding a mean cross-validation score of 80% with a standard deviation of 0.02.\n\n### Final Remarks\n\n- The Gaussian Naive Bayes model shows a consistent accuracy of 80% across different cross-validation folds.\n- The model's precision, recall, and f1-score indicate that it performs well, though there is room for improvement, particularly in recall for the positive class.\n- The addition of engineered features and handling of class imbalance using SMOTE + TOMEK contributed to the model's performance.\n- The model's learning curve suggests it generalizes well without overfitting or underfitting.\n- **Note**: The high standard deviation in cross-validation scores for some values of k indicates variability in model performance across different folds. This suggests that the model's accuracy may vary depending on the specific data split, highlighting the importance of robust cross-validation.\n\nOverall, this kernel provides a comprehensive approach to predicting Titanic survival, showcasing the importance of data preprocessing, feature engineering, and model evaluation.\n","metadata":{}}]}