{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":72528,"databundleVersionId":7946462,"sourceType":"competition"},{"sourceId":8761080,"sourceType":"datasetVersion","datasetId":5263731},{"sourceId":8761082,"sourceType":"datasetVersion","datasetId":5263738}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Titanic Survival Prediction\n\n## Introduction\n\nThis kernel demonstrates the steps to preprocess the data, engineer new features, handle missing values, and apply machine learning techniques to predict the survival of Titanic passengers. We use a Decision Tree classifier and evaluate its performance using various metrics and cross-validation techniques.\n","metadata":{}},{"cell_type":"markdown","source":"## 1. Data Loading and Initial Analysis\n\nWe start by loading the necessary packages and the dataset, then conduct an initial analysis to understand the data structure and identify missing values.\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, RobustScaler, PowerTransformer, QuantileTransformer, PolynomialFeatures\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, KFold,learning_curve\nfrom sklearn.feature_selection import mutual_info_classif, mutual_info_regression\nfrom imblearn.combine import SMOTETomek\nfrom scipy import stats\nfrom scipy.stats import skew, kurtosis\nimport warnings\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_curve, auc\nimport string\n\n#for Age forecast\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import VotingRegressor\n\n\n# Suppress warnings\nwarnings.filterwarnings('ignore')\n\n\n# Set plot style for seaborn\nsns.set(style=\"whitegrid\")\n\nprint('Packages were loaded')\n\n\ntrain = pd.read_csv('/kaggle/input/2024-sce-ml-course/train_2022.csv')\ntest = pd.read_csv('/kaggle/input/2024-sce-ml-course/test_2022.csv')\n\nprint(train.info())\nprint(\"-\"*100)\nprint(test.info())\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-15T18:29:34.205618Z","iopub.execute_input":"2024-07-15T18:29:34.206050Z","iopub.status.idle":"2024-07-15T18:29:34.217909Z","shell.execute_reply.started":"2024-07-15T18:29:34.206016Z","shell.execute_reply":"2024-07-15T18:29:34.216652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualizing Data Characteristics\nWe visualize the data to understand its structure, identify missing values, and observe feature distributions and correlations.\n#### Graphs are ploted by q:\n1. Missing Values Distribution\n1. Histograms of Numerical Features\n1. Feature Correlation Matrix\n1. Summary Statistics of Numerical Features\n1. Categorical Feature Distribution\n\n","metadata":{}},{"cell_type":"code","source":"\ntrain[\"Survived_binary\"]=train[\"Survived\"].map({'T':1,'F': 0})\n\n\ndef visualize_data_analysis(df, df_name, q=[1, 2, 3, 4, 5], encoded_categorical_cols=['Survived', 'Sex', 'Embarked', 'Deck', 'Title', 'Title_Category', 'AgeGroup', 'Married']):\n    df_temp = df.copy()\n    \n    if 1 in q:\n        plt.figure(figsize=(10, 6))\n        sns.heatmap(df_temp.isnull(), cbar=False, cmap='viridis')\n        plt.title(f'Missing Values Distribution of {df_name}')\n        plt.show()\n\n    if 2 in q:\n        numeric_df = df_temp.select_dtypes(include=[np.number])\n        if not numeric_df.empty:\n            numeric_df.hist(bins=20, figsize=(20, 15), edgecolor='black')\n            plt.suptitle(f'Histograms of Numerical Features of {df_name}')\n            plt.show()\n\n    if 3 in q:\n        numeric_df = df_temp.select_dtypes(include=[np.number])\n        plt.figure(figsize=(25, 20))\n        sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm', fmt='.2f')\n        plt.title(f'Feature Correlation Matrix of {df_name}')\n        plt.show()\n    \n    if 4 in q:\n        numeric_df = df_temp.select_dtypes(include=[np.number])\n        if 'PassengerId' in numeric_df.columns:\n            numeric_df = numeric_df.drop(columns=['PassengerId'])\n        summary = numeric_df.describe().T\n        summary['median'] = numeric_df.median()\n        summary['skew'] = numeric_df.skew()\n        summary['kurtosis'] = numeric_df.kurtosis()\n\n        summary = summary[['mean', 'median', 'std', 'min', 'max', 'skew', 'kurtosis']]\n        summary.columns = ['Mean', 'Median', 'Std Dev', 'Min', 'Max', 'Skew', 'Kurtosis']\n\n        plt.figure(figsize=(12, 8))\n        sns.heatmap(summary, annot=True, cmap='coolwarm', cbar=False, fmt='.2f', linewidths=0.5)\n        plt.title(f'Summary Statistics of Numeric Features of {df_name}')\n        plt.show()\n        \n    if 5 in q:        \n        # Plot categorical features with pie and bar charts\n        for col in encoded_categorical_cols:\n            if col in df_temp.columns and df_temp[col].nunique() <= 7:  # Ensure the column exists in df_temp and has <= 7 unique values\n                fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n                # Use ax.pie directly to get wedges, texts, and autotexts\n                wedges, texts, autotexts = axes[0].pie(\n                    df_temp[col].value_counts(), autopct='%1.1f%%', colors=['#66b3ff', '#99ff99', '#ffcc99'], startangle=90\n                )\n                for text, autotext in zip(texts, autotexts):\n                    text.set_rotation(45)  # Rotate the labels\n                    autotext.set_rotation(45)\n                axes[0].set_title(f'{col} Distribution (Pie)')\n                axes[0].set_ylabel('')\n\n                sns.countplot(data=df_temp, x=col, ax=axes[1])\n                axes[1].set_title(f'{col} Distribution (Bar)')\n                plt.tight_layout()\n                plt.show()\n\n\n        \n# Visualize training data characteristics\nvisualize_data_analysis(train, \"Train Data\",q=[1,2,3,4])\nvisualize_data_analysis(test, \"Test Data\", q=[1,4])","metadata":{"execution":{"iopub.status.busy":"2024-07-15T18:29:34.220314Z","iopub.execute_input":"2024-07-15T18:29:34.221165Z","iopub.status.idle":"2024-07-15T18:29:40.183733Z","shell.execute_reply.started":"2024-07-15T18:29:34.221121Z","shell.execute_reply":"2024-07-15T18:29:40.182138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Feature Engineering and Handling Missing Data\n### Feature Engineering\nWe create new features to enhance the predictive power of our model. These features include:\n\n1. Deck: Extracted from the Cabin column and grouped into categories.\n1. Title: Extracted from the Name column and grouped into common categories.\n1. FamilySize: Calculated as the sum of SibSp and Parch plus 1.\n1. IsAlone: Indicates whether the passenger is traveling alone.\n1. FarePerPerson: Calculated as Fare divided by FamilySize.\n1. TicketPrefix and TicketNumber: Extracted from the Ticket column.\n1. Title_Category: Maps titles to broader categories.\n1. Sex_binary: Binary encoding of the Sex column.\n1. GroupSize: Number of people traveling with the same ticket.\n1. IsInGroup: Indicates whether the passenger is part of a group.\n1. ParchSibSp: Product of Parch and SibSp.\n1. FareBin: Binned version of FarePerPerson.\n1. CabinCount: Number of cabins assigned to the passenger.","metadata":{}},{"cell_type":"code","source":"def extract_ticket_features(ticket):\n    ticket = ticket.replace('.', '').replace('/', '').split()\n    ticket_prefix = ticket[0] if not ticket[0].isdigit() else 'NoPrefix'\n    ticket_number = ticket[-1] if ticket[-1].isdigit() else 'NoNumber'\n    return ticket_prefix, ticket_number\n\ndef feature_engineering_encoding_missingData(df1, df2):\n    combine = [df1.copy(), df2.copy()]\n    for i in range(len(combine)):\n        df_temp = combine[i]\n        \n        # When I googled Stone, Mrs. George Nelson (Martha Evelyn), I found that she embarked from S (Southampton) with her maid Amelie Icard, in this page Martha Evelyn Stone: Titanic Survivor.\n        df_temp['Embarked'] = df_temp['Embarked'].fillna('S')\n\n        # Fill missing 'Cabin' with 'Unknown'\n        df_temp['Cabin'].fillna('Unknown', inplace=True)\n\n        # Create deck feature extracted from the cabin number and replace it to be align with Pclass\n        df_temp['Deck'] = df_temp['Cabin'].apply(lambda x: x[0])\n        df_temp['Deck'] = df_temp['Deck'].replace(['A', 'B', 'C'], 'ABC')\n        df_temp['Deck'] = df_temp['Deck'].replace(['D', 'E'], 'DE')\n        df_temp['Deck'] = df_temp['Deck'].replace(['F', 'G'], 'FG')\n        \n\n        # Create new features\n        df_temp['Title'] = df_temp['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())\n        df_temp['Title'] = df_temp['Title'].replace(['Lady', 'Countess', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Master', 'Jonkheer', 'Dona'], 'Rare')\n        df_temp['Title'] = df_temp['Title'].replace(['Mlle', 'Ms'], 'Miss')\n        df_temp['Title'] = df_temp['Title'].replace('Mme', 'Mrs')\n        df_temp['Married'] = df_temp['Title'].apply(lambda x: 1 if x == 'Mrs' else 0)\n        df_temp['Title'] = df_temp['Title'].apply(lambda x: 'Rare' if x not in ['Mr', 'Mrs', 'Miss'] else x)\n\n        df_temp['FamilySize'] = df_temp['SibSp'] + df_temp['Parch'] + 1\n        df_temp['IsAlone'] = df_temp['FamilySize'].apply(lambda x: 1 if x == 1 else 0)\n        \n        # Median of a Fare satisying condition([3][0][0] -- 3=Pclass,0=Parch,SibSp) based on the only missing sample\n        # Filling the missing value in Fare with the median Fare of 3rd class alone passenger\n        med_fare = df_temp.groupby(['Pclass', 'Parch', 'SibSp'])['Fare'].median()[3][0][0]\n        df_temp['Fare'] = df_temp['Fare'].fillna(med_fare)\n        \n        df_temp['FarePerPerson'] = df_temp['Fare'] / df_temp['FamilySize']\n        df_temp = df_temp.drop(columns=['Fare'])\n\n        df_temp[['TicketPrefix', 'TicketNumber']] = df_temp['Ticket'].apply(lambda x: pd.Series(extract_ticket_features(x)))\n\n        title_mapping = {'Mr': 'Common', 'Mrs': 'Common', 'Miss': 'Common', 'Master': 'Common', 'Rare': 'Royalty'}\n        df_temp['Title_Category'] = df_temp['Title'].map(title_mapping)\n\n        if 'Survived' in df_temp.columns:\n            df_temp['Survived_binary'] = df_temp['Survived'].map({'T': 1, 'F': 0})\n\n        df_temp['Sex_binary'] = df_temp['Sex'].map({'male': 0, 'female': 1})\n        df_temp['GroupSize'] = df_temp.groupby('Ticket')['Ticket'].transform('count')\n        df_temp['IsInGroup'] = df_temp['GroupSize'].apply(lambda x: 1 if x > 1 else 0)\n\n        # Apply one-hot encoding\n        one_hot_encoded = pd.get_dummies(df_temp[['Title','Married']], drop_first=True)\n        one_hot_encoded = pd.get_dummies(df_temp[['Embarked','Sex_binary','Deck',]], drop_first=False)\n\n        one_hot_encoded = one_hot_encoded.astype(int)\n        df_temp = df_temp.drop(columns=['Sex_binary', 'IsAlone' ])\n        df_temp = pd.concat([df_temp, one_hot_encoded], axis=1)\n\n        df_temp['ParchSibSp'] = df_temp['Parch'] * df_temp['SibSp']\n        \n        df_temp['FareBin'] = pd.qcut(df_temp['FarePerPerson'], 13, labels=False)\n        \n        df_temp['CabinCount'] = df_temp['Cabin'].apply(lambda x: len(x.split()) if pd.notna(x) else 0)\n        \n        # Mapping Family Size\n        family_map = {1: 'Alone', 2: 'Small', 3: 'Small', 4: 'Small', 5: 'Medium', 6: 'Medium', 7: 'Large', 8: 'Large', 9: 'Large', 10: 'Large', 11: 'Large'}\n        df_temp['Family_Size_Grouped'] = df_temp['FamilySize'].map(family_map)\n        \n        combine[i] = df_temp\n\n    return combine[0], combine[1]\n\ntrain_encoded, test_encoded = feature_engineering_encoding_missingData(train, test)\n\nvisualize_data_analysis(train_encoded, \"Train Data\",q=[4])\n","metadata":{"execution":{"iopub.status.busy":"2024-07-15T18:29:40.185331Z","iopub.execute_input":"2024-07-15T18:29:40.185718Z","iopub.status.idle":"2024-07-15T18:29:41.623511Z","shell.execute_reply.started":"2024-07-15T18:29:40.185686Z","shell.execute_reply":"2024-07-15T18:29:41.622242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Explore Features Distributions Segmented with Survived\n\nTo understand how different features affect the survival rate, we explore the distributions of categorical features segmented by the `Survived` column. This helps us visualize the impact of each feature on survival.\n","metadata":{}},{"cell_type":"code","source":"cat_features = ['Embarked', 'Parch', 'Pclass', 'Sex', 'SibSp', 'Deck','Title','IsInGroup','Married','GroupSize','FareBin','Family_Size_Grouped']\n\nfig, axs = plt.subplots(ncols=2, nrows=3, figsize=(20, 20))\nplt.subplots_adjust(right=1.5, top=1.25)\n\nfor i, feature in enumerate(cat_features, 1):    \n    plt.subplot(4, 3, i)\n    sns.countplot(x=feature, hue='Survived', data=train_encoded)\n    \n    plt.xlabel('{}'.format(feature), size=20, labelpad=15)\n    plt.ylabel('Passenger Count', size=20, labelpad=15)    \n    plt.tick_params(axis='x', labelsize=20)\n    plt.tick_params(axis='y', labelsize=20)\n    \n    plt.legend(['Not Survived', 'Survived'], loc='upper center', prop={'size': 18})\n    plt.title('Count of Survival in {} Feature'.format(feature), size=20, y=1.05)\n    fig.subplots_adjust(hspace=0.4, wspace=0.3)  # Adjust the values as needed\n\nplt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-15T18:29:41.626427Z","iopub.execute_input":"2024-07-15T18:29:41.626884Z","iopub.status.idle":"2024-07-15T18:29:44.600115Z","shell.execute_reply.started":"2024-07-15T18:29:41.626844Z","shell.execute_reply":"2024-07-15T18:29:44.598796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Label Encoding for Categorical Features\nWe need to convert categorical features into numerical values for the machine learning models to process them. We use LabelEncoder to transform the categorical columns.","metadata":{}},{"cell_type":"code","source":"le = LabelEncoder()\ncategorical_cols = ['Embarked', 'Deck', 'Title', 'TicketPrefix', 'Title_Category','Family_Size_Grouped','TicketNumber']\ncombine = [train_encoded, test_encoded]\nfor df_temp in combine:\n    for col in categorical_cols:\n            df_temp[col] = le.fit_transform(df_temp[col].astype(str))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Forecasting Missing Ages\nMissing values in the Age column can be significant for our model. We use a combination of mutual information, decision trees, and k-nearest neighbors to predict and fill in the missing ages.","metadata":{}},{"cell_type":"markdown","source":"### Separating Rows with Missing and Non-missing Ages\n\n","metadata":{}},{"cell_type":"code","source":"def separate_missing_ages(df):\n    df_missing_age = df[df['Age'].isnull()]\n    df_non_missing_age = df[df['Age'].notnull()]\n    return df_missing_age, df_non_missing_age","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Selecting Best Features Using Mutual Information\n\n","metadata":{}},{"cell_type":"code","source":"\ndef select_features(X, y, num_features, plot=True):\n    # Ensure all features are numeric\n    X_numeric = X.select_dtypes(include=[np.number])\n    mi = mutual_info_regression(X_numeric, y)\n    mi_series = pd.Series(mi, index=X_numeric.columns)\n    selected_features = mi_series.sort_values(ascending=False).head(num_features).index.tolist()\n\n    if plot:\n        plt.figure(figsize=(10, 6))\n        mi_series.sort_values(ascending=False).plot(kind='bar')\n        plt.title('Mutual Information of Features with Age')\n        plt.xlabel('Features')\n        plt.ylabel('Mutual Information Score')\n        plt.show()\n\n    return sorted(selected_features)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training Age Predictors\n\n","metadata":{}},{"cell_type":"code","source":"# Define the function to train age predictors\ndef train_age_predictors(X, y):\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n\n    param_grid_dt = {\n        'max_depth': [None, 10, 20, 30],\n        'min_samples_split': [2, 5, 10],\n        'min_samples_leaf': [1, 2, 4],\n        'max_features': ['auto', 'sqrt', 'log2']\n    }\n\n    param_grid_knn = {\n        'n_neighbors': [3, 5, 7, 9, 11],\n        'weights': ['uniform', 'distance'],\n        'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n    }\n\n    models = {\n        'Decision Tree': (DecisionTreeRegressor(), param_grid_dt),\n        'K-Nearest Neighbors': (KNeighborsRegressor(), param_grid_knn)\n    }\n\n    best_models = {}\n    for name in models:\n        model, param_grid = models[name]\n        grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error')\n        grid_search.fit(X_scaled, y)\n        best_models[name] = grid_search.best_estimator_\n        print(f\"Best parameters for {name}: {grid_search.best_params_}\")\n        print(f\"Best neg MSE score for {name}: {grid_search.best_score_}\")\n\n    voting_regressor = VotingRegressor(estimators=[\n        ('dt', best_models['Decision Tree']),\n        ('knn', best_models['K-Nearest Neighbors'])\n    ])\n    voting_regressor.fit(X_scaled, y)\n\n    return best_models, voting_regressor, scaler","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Predicting Missing Ages\n\n","metadata":{}},{"cell_type":"code","source":"# Function to predict missing ages\ndef predict_missing_ages(voting_regressor, scaler, df_missing_age, features):\n    X_missing = df_missing_age[features].sort_index(axis=1)\n    X_missing_scaled = scaler.transform(X_missing)\n    df_missing_age['Age'] = voting_regressor.predict(X_missing_scaled)\n    return df_missing_age","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Handling Skewness in Data\nWe address the skewness in numerical features by applying transformations.\n\n#### Identifying Skewed Features for KNN model \n#### Transforming Skewed Features\n#### Visualizing Distributions\n#### Processing and Transforming the Data\n\n\n\n","metadata":{}},{"cell_type":"code","source":"def get_skewed_features(df, threshold=0.5, high_skew_threshold=1.0):\n    df_copy = df.copy()\n    numerical_features = df_copy.select_dtypes(include=[np.number]).columns.tolist()\n    excluded_features = ['Sex_binary', 'IsChild', 'Married', 'Title_Category', 'PassengerId']\n    numerical_features = [feature for feature in numerical_features if feature not in excluded_features]\n    \n    skew_df = df_copy[numerical_features].apply(lambda x: x.skew())\n    kurtosis_df = df_copy[numerical_features].apply(lambda x: x.kurtosis())\n    skew_df = skew_df.sort_values(ascending=False)\n    skew_df = skew_df.reset_index()\n    skew_df.columns = ['Feature', 'SkewFactor']\n    kurtosis_df = kurtosis_df.reset_index()\n    kurtosis_df.columns = ['Feature', 'Kurtosis']\n    skew_kurtosis_df = pd.merge(skew_df, kurtosis_df, on='Feature')\n    highly_skewed_features = list(skew_kurtosis_df[skew_kurtosis_df['SkewFactor'] > high_skew_threshold]['Feature'])\n    moderately_skewed_features = list(skew_kurtosis_df[(skew_kurtosis_df['SkewFactor'] > threshold) & (skew_kurtosis_df['SkewFactor'] <= high_skew_threshold)]['Feature'])\n    non_skewed_features = list(set(numerical_features) - set(moderately_skewed_features) - set(highly_skewed_features))\n    return highly_skewed_features, moderately_skewed_features, non_skewed_features, skew_kurtosis_df\n\ndef transform_highly_skewed_features(df, highly_skewed_features):\n    df_copy = df.copy()\n    for col in highly_skewed_features:\n        try:\n            transformed_data, _ = stats.boxcox(df_copy[col] + 1)  # Ensure positive values\n            df_copy[col] = transformed_data\n        except Exception:\n            df_copy[col] = np.log1p(df_copy[col])\n    return df_copy\n\ndef transform_moderately_skewed_features(df, moderately_skewed_features):\n    df_copy = df.copy()\n    for col in moderately_skewed_features:\n        df_copy[col] = np.log1p(df_copy[col])\n    return df_copy\n\ndef transform_non_skewed_features(df, non_skewed_features):\n    df_copy = df.copy()\n    scaler = StandardScaler()\n    df_copy[non_skewed_features] = scaler.fit_transform(df_copy[non_skewed_features])\n    return df_copy\n\ndef skew_groups(df_train, df_test):\n    combine = [df_train.copy(), df_test.copy()]\n    for i in range(len(combine)):\n        df_temp = combine[i]\n        highly_skewed_features, moderately_skewed_features, non_skewed_features, skew_kurtosis_df = get_skewed_features(df_temp, threshold=0.5, high_skew_threshold=1.0)\n        df_temp = transform_highly_skewed_features(df_temp, highly_skewed_features)\n        df_temp = transform_moderately_skewed_features(df_temp, moderately_skewed_features)\n        df_temp = transform_non_skewed_features(df_temp, non_skewed_features)\n        combine[i] = df_temp\n    return combine[0], combine[1]\n\ndef visualize_distributions(df, highly_skewed_features, moderately_skewed_features, skew_kurtosis_df, threshold=0.1):\n    df_copy = df.copy()\n    significant_features = highly_skewed_features + moderately_skewed_features\n    \n    num_features = len(significant_features)\n    if num_features == 0:\n        print(\"No significant changes in skewness detected.\")\n        return\n    \n    fig, axes = plt.subplots(nrows=num_features, ncols=2, figsize=(14, 6 * num_features))\n    \n    for i, col in enumerate(significant_features):\n        original_skewness = df_copy[col].skew()\n        original_kurtosis = skew_kurtosis_df[skew_kurtosis_df['Feature'] == col]['Kurtosis'].values[0]\n        sns.histplot(df_copy[col].dropna(), kde=True, ax=axes[i, 0], color='blue', bins=30)\n        axes[i, 0].set_title(f'Before: {col} (Skewness: {original_skewness:.2f}, Kurtosis: {original_kurtosis:.2f})', fontsize=14)\n        axes[i, 0].set_xlabel('')\n        axes[i, 0].set_ylabel('')\n    \n    # Apply the transformations\n    df_copy = transform_highly_skewed_features(df_copy, highly_skewed_features)\n    df_copy = transform_moderately_skewed_features(df_copy, moderately_skewed_features)\n    df_copy = transform_non_skewed_features(df_copy, list(set(df_copy.select_dtypes(include=[np.number]).columns) - set(significant_features)))\n    \n    for i, col in enumerate(highly_skewed_features):\n        transformed_skewness = df_copy[col].skew()\n        transformed_kurtosis = df_copy[col].kurtosis()\n        sns.histplot(df_copy[col].dropna(), kde=True, ax=axes[i, 1], color='green', bins=30)\n        axes[i, 1].set_title(f'After (Box-Cox): {col} (Skewness: {transformed_skewness:.2f}, Kurtosis: {transformed_kurtosis:.2f})', fontsize=14)\n        axes[i, 1].set_xlabel('')\n        axes[i, 1].set_ylabel('')\n    \n    for i, col in enumerate(moderately_skewed_features, len(highly_skewed_features)):\n        transformed_skewness = df_copy[col].skew()\n        transformed_kurtosis = df_copy[col].kurtosis()\n        sns.histplot(df_copy[col].dropna(), kde=True, ax=axes[i, 1], color='green', bins=30)\n        axes[i, 1].set_title(f'After (Log): {col} (Skewness: {transformed_skewness:.2f}, Kurtosis: {transformed_kurtosis:.2f})', fontsize=14)\n        axes[i, 1].set_xlabel('')\n        axes[i, 1].set_ylabel('')\n    \n    plt.tight_layout()\n    plt.show()\n    return df_copy\n\n\n\n# Separate rows with missing and non-missing ages for training data\ndf_missing_age_train, df_non_missing_age_train = train_encoded[train_encoded['Age'].isnull()], train_encoded[train_encoded['Age'].notnull()]\ndf_missing_age_test, df_non_missing_age_test = test_encoded[test_encoded['Age'].isnull()], test_encoded[test_encoded['Age'].notnull()]\n\n# Prepare training data\nX_train = df_non_missing_age_train.drop(columns=['Age'])\ny_train = df_non_missing_age_train['Age']\n\n# Select best features using MI\nselected_features = select_features(X_train, y_train, num_features=7)\nX_train_selected = X_train[selected_features]\n\n# Train the models\n_, voting_regressor, scaler = train_age_predictors(X_train_selected, y_train)\n\n# Ensure the selected features are present in both train and test datasets\ncommon_features = list(set(selected_features) & set(df_missing_age_test.columns))\n\n# Scale the data\nX_train_selected_scaled = scaler.fit_transform(X_train_selected)\n\n# Train the voting regressor on scaled data\nvoting_regressor.fit(X_train_selected_scaled, y_train)\n\n# Process skewness for missing age data\ndf_missing_age_train, df_missing_age_test = skew_groups(df_missing_age_train, df_missing_age_test)\n\n\n# Predict missing ages for training and test data\ndf_missing_age_train = predict_missing_ages(voting_regressor, scaler, df_missing_age_train, common_features)\ndf_missing_age_test = predict_missing_ages(voting_regressor, scaler, df_missing_age_test, common_features)\n\n# Combine the non-missing and newly predicted ages for training and test data\ntrain_encoded['Age'] = pd.concat([df_non_missing_age_train['Age'], df_missing_age_train['Age']])\ntest_encoded['Age'] = pd.concat([df_non_missing_age_test['Age'], df_missing_age_test['Age']])\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-15T18:29:44.601982Z","iopub.execute_input":"2024-07-15T18:29:44.602455Z","iopub.status.idle":"2024-07-15T18:29:51.052104Z","shell.execute_reply.started":"2024-07-15T18:29:44.602414Z","shell.execute_reply":"2024-07-15T18:29:51.050629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Creating Age-Related Features\n\nWe categorize ages into different groups (e.g., Baby, Child, Teenager, etc.) and create a new feature representing these groups. This helps the model understand the age distribution better.\n\n","metadata":{}},{"cell_type":"code","source":"def age_related_features(df):\n    age_bins = [0, 2, 12, 18, 25, 35, 60, np.inf]\n    age_labels = [1, 2, 3, 4, 5, 6, 7]\n    df['AgeGroupNum'] = pd.cut(df['Age'], bins=age_bins, labels=age_labels).astype(int)\n\n    age_labels = ['Baby', 'Child', 'Teenager', 'Young Adult', 'Adult', 'Middle Aged', 'Senior']\n    df['AgeGroup'] = pd.cut(df['Age'], bins=age_bins, labels=age_labels)\n    df['age_sex_string'] = (df['AgeGroup'].str.lower() + ' - ' + df['Sex'].str.lower())\n\n    label_encoder = LabelEncoder()\n    df['age_sex_encoded'] = label_encoder.fit_transform(df['age_sex_string'])\n    df['IsChild'] = df['Age'].apply(lambda x: 1 if x < 18 else 0)\n    \n\n\n\nage_related_features(train_encoded)\nage_related_features(test_encoded)\n\nvisualize_data_analysis(train_encoded, \"Train Data\", [4, 5],encoded_categorical_cols=['Survived','AgeGroup','IsChild','age_sex_string'])\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7. final Standardization\n\n1. Age: Standard Scaling (normal distribution with reasonable spread).\n1. FarePerPerson: Log Transformation (high skewness and spread).\n1. TicketNumber: Standard Scaling (high range and spread).\n\n* drop sex binary since sex_age is better scored by MI","metadata":{}},{"cell_type":"code","source":"train_encoded = train_encoded.drop(columns=['Sex_binary','Married'])\ntest_encoded = test_encoded.drop(columns=['Sex_binary','Married'])\n\ndef log_transform(x):\n    return np.log1p(x)\n\n\ndef sqrt_transform(x):\n    return np.sqrt(x)\n\ndef plot_distributions(original_df, transformed_df, features, title_prefix):\n    for feature in features:\n        fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n        # Original data\n        sns.histplot(original_df[feature].dropna(), kde=True, ax=axes[0])\n        axes[0].set_title(f'{title_prefix} - Original {feature}')\n        axes[0].text(1.02, 0.6, f\"Mean: {original_df[feature].mean():.2f}\\n\"\n                                f\"Median: {original_df[feature].median():.2f}\\n\"\n                                f\"Std: {original_df[feature].std():.2f}\\n\"\n                                f\"Skew: {skew(original_df[feature].dropna()):.2f}\\n\"\n                                f\"Kurtosis: {kurtosis(original_df[feature].dropna()):.2f}\",\n                                transform=axes[0].transAxes)\n\n        # Transformed data\n        sns.histplot(transformed_df[feature].dropna(), kde=True, ax=axes[1])\n        axes[1].set_title(f'{title_prefix} - Transformed {feature}')\n        axes[1].text(1.02, 0.6, f\"Mean: {transformed_df[feature].mean():.2f}\\n\"\n                                f\"Median: {transformed_df[feature].median():.2f}\\n\"\n                                f\"Std: {transformed_df[feature].std():.2f}\\n\"\n                                f\"Skew: {skew(transformed_df[feature].dropna()):.2f}\\n\"\n                                f\"Kurtosis: {kurtosis(transformed_df[feature].dropna()):.2f}\",\n                                transform=axes[1].transAxes)\n\n        plt.tight_layout()\n        plt.show()\n\n\n\ndef transform_and_plot(train_df, test_df, verbose=1):\n    numerical_features = ['Age','FarePerPerson', 'TicketNumber'] #'CabinCount','SibSp', 'Parch', 'FamilySize',  'GroupSize', 'ParchSibSp', \n\n    # Copy original data for plotting\n    original_train = train_df[numerical_features].copy()\n\n    # Initialize transformers\n    transformers = {\n        'Age': StandardScaler(),\n        'FarePerPerson': log_transform,\n        'TicketNumber': QuantileTransformer(output_distribution='normal'),\n\n    }\n\n    # Apply the transformations to train data\n    for feature, transformer in transformers.items():\n        if feature in ['FarePerPerson']:\n            train_df[feature] = transformer(train_df[feature])\n        else:\n            train_df[feature] = transformer.fit_transform(train_df[[feature]])\n\n    # Apply the same transformations to test data\n    for feature, transformer in transformers.items():\n        if feature in ['FarePerPerson']:\n            test_df[feature] = transformer(test_df[feature])\n        else:\n            test_df[feature] = transformer.transform(test_df[[feature]])\n\n    if verbose:\n        plot_distributions(original_train, train_df, numerical_features, 'Train Data')\n\n    return train_df, test_df\n\n# Apply transformations and plot\ntrain_encoded, test_encoded = transform_and_plot(train_encoded, test_encoded, verbose=1)\nvisualize_data_analysis(train_encoded, \"Train Data\", [3,4])\n\n\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-15T18:29:51.054021Z","iopub.execute_input":"2024-07-15T18:29:51.054533Z","iopub.status.idle":"2024-07-15T18:29:59.682316Z","shell.execute_reply.started":"2024-07-15T18:29:51.054490Z","shell.execute_reply":"2024-07-15T18:29:59.680948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 8. Creating Survival Rate Features\n\nWe create new features that represent the survival rate based on the family name and ticket number. This can help improve the model's prediction accuracy by providing additional context on survival probabilities.\n\n**Family and Ticket Survival Rates**\n\nWe calculate the survival rates for families and tickets that appear in both the training and test datasets. This provides a survival probability based on the family name and ticket number, which can be a strong predictor of individual survival.\n\n**Combining Family and Ticket Survival Rates**\n\nWe create new features by combining family and ticket survival rates. This provides a more comprehensive survival rate feature that captures both family and ticket information.\n","metadata":{}},{"cell_type":"code","source":"\n\n# Extract the family surname from a given name string.\ndef extract_surname(data):\n    families = []\n    for i in range(len(data)):\n        name = data.iloc[i]\n        if '(' in name:\n            name_no_bracket = name.split('(')[0]\n        else:\n            name_no_bracket = name\n            \n        family = name_no_bracket.split(',')[0]\n        for c in string.punctuation:\n            family = family.replace(c, '').strip()\n            \n        families.append(family)\n    return families\n\n# Ensure 'FamilySize' is numeric\ntrain_encoded['FamilySize'] = pd.to_numeric(train_encoded['FamilySize'], errors='coerce')\ntest_encoded['FamilySize'] = pd.to_numeric(test_encoded['FamilySize'], errors='coerce')\n\n# Extracting Ticket_Frequencies\ntrain_encoded['Ticket_Frequency'] = train_encoded.groupby('Ticket')['Ticket'].transform('count')\ntest_encoded['Ticket_Frequency'] = test_encoded.groupby('Ticket')['Ticket'].transform('count')\n\n# Extracting family names\ntrain_encoded['Family'] = extract_surname(train_encoded['Name'])\ntest_encoded['Family'] = extract_surname(test_encoded['Name'])\n\n# Creating a list of families and tickets that are occurring in both training and test set\nnon_unique_families = [x for x in train_encoded['Family'].unique() if x in test_encoded['Family'].unique()]\nnon_unique_tickets = [x for x in train_encoded['Ticket'].unique() if x in test_encoded['Ticket'].unique()]\n\n# Ensure 'Ticket_Frequency' is numeric\ntrain_encoded['Ticket_Frequency'] = pd.to_numeric(train_encoded['Ticket_Frequency'], errors='coerce')\ntest_encoded['Ticket_Frequency'] = pd.to_numeric(test_encoded['Ticket_Frequency'], errors='coerce')\n\n\n# Calculate survival rates, ensuring only numeric columns are used\ndf_family_survival_rate = train_encoded.groupby('Family')[['Survived_binary', 'FamilySize']].median()\ndf_ticket_survival_rate = train_encoded.groupby('Ticket')[['Survived_binary', 'Ticket_Frequency']].median()\n\n\n\nfamily_rates = {}\nticket_rates = {}\n\n# Check columns and process the DataFrame accordingly\nif 'FamilySize' in df_family_survival_rate.columns:\n    for i in range(len(df_family_survival_rate)):\n        if df_family_survival_rate.index[i] in non_unique_families and df_family_survival_rate.iloc[i]['FamilySize'] > 1:\n            family_rates[df_family_survival_rate.index[i]] = df_family_survival_rate.iloc[i]['Survived_binary']\n\nif 'Ticket_Frequency' in df_ticket_survival_rate.columns:\n    for i in range(len(df_ticket_survival_rate)):\n        if df_ticket_survival_rate.index[i] in non_unique_tickets and df_ticket_survival_rate.iloc[i]['Ticket_Frequency'] > 1:\n            ticket_rates[df_ticket_survival_rate.index[i]] = df_ticket_survival_rate.iloc[i]['Survived_binary']\n\nmean_survival_rate = np.mean(train_encoded['Survived_binary'])\n\ntrain_family_survival_rate = []\ntrain_family_survival_rate_NA = []\ntest_family_survival_rate = []\ntest_family_survival_rate_NA = []\n\nfor i in range(len(train_encoded)):\n    if train_encoded['Family'][i] in family_rates:\n        train_family_survival_rate.append(family_rates[train_encoded['Family'][i]])\n        train_family_survival_rate_NA.append(1)\n    else:\n        train_family_survival_rate.append(mean_survival_rate)\n        train_family_survival_rate_NA.append(0)\n        \nfor i in range(len(test_encoded)):\n    if test_encoded['Family'].iloc[i] in family_rates:\n        test_family_survival_rate.append(family_rates[test_encoded['Family'].iloc[i]])\n        test_family_survival_rate_NA.append(1)\n    else:\n        test_family_survival_rate.append(mean_survival_rate)\n        test_family_survival_rate_NA.append(0)\n        \ntrain_encoded['Family_Survival_Rate'] = train_family_survival_rate\ntrain_encoded['Family_Survival_Rate_NA'] = train_family_survival_rate_NA\ntest_encoded['Family_Survival_Rate'] = test_family_survival_rate\ntest_encoded['Family_Survival_Rate_NA'] = test_family_survival_rate_NA\n\ntrain_ticket_survival_rate = []\ntrain_ticket_survival_rate_NA = []\ntest_ticket_survival_rate = []\ntest_ticket_survival_rate_NA = []\n\nfor i in range(len(train_encoded)):\n    if train_encoded['Ticket'][i] in ticket_rates:\n        train_ticket_survival_rate.append(ticket_rates[train_encoded['Ticket'][i]])\n        train_ticket_survival_rate_NA.append(1)\n    else:\n        train_ticket_survival_rate.append(mean_survival_rate)\n        train_ticket_survival_rate_NA.append(0)\n        \nfor i in range(len(test_encoded)):\n    if test_encoded['Ticket'].iloc[i] in ticket_rates:\n        test_ticket_survival_rate.append(ticket_rates[test_encoded['Ticket'].iloc[i]])\n        test_ticket_survival_rate_NA.append(1)\n    else:\n        test_ticket_survival_rate.append(mean_survival_rate)\n        test_ticket_survival_rate_NA.append(0)\n        \ntrain_encoded['Ticket_Survival_Rate'] = train_ticket_survival_rate\ntrain_encoded['Ticket_Survival_Rate_NA'] = train_ticket_survival_rate_NA\ntest_encoded['Ticket_Survival_Rate'] = test_ticket_survival_rate\ntest_encoded['Ticket_Survival_Rate_NA'] = test_ticket_survival_rate_NA\n\nfor df in [train_encoded, test_encoded]:\n    df['Survival_Rate'] = (df['Ticket_Survival_Rate'] + df['Family_Survival_Rate']) / 2\n    df['Survival_Rate_NA'] = (df['Ticket_Survival_Rate_NA'] + df['Family_Survival_Rate_NA']) / 2\n    \nvisualize_data_analysis(train_encoded, \"Train Data\",q=[5],encoded_categorical_cols=['Ticket_Survival_Rate_NA','Survival_Rate_NA'])\n","metadata":{"execution":{"iopub.status.busy":"2024-07-15T18:29:59.684081Z","iopub.execute_input":"2024-07-15T18:29:59.684587Z","iopub.status.idle":"2024-07-15T18:30:01.026851Z","shell.execute_reply.started":"2024-07-15T18:29:59.684542Z","shell.execute_reply":"2024-07-15T18:30:01.025513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 9. Apply SmoteTomek and train model\n","metadata":{}},{"cell_type":"code","source":"\n\n\n# # Function to select features based on mutual information for the survival prediction task\ndef select_survival_features(X, y, num_features, plot=True):\n    # Select only numeric features\n    X_numeric = X.select_dtypes(include=[np.number])\n    \n    # Calculate mutual information scores\n    mi = mutual_info_classif(X_numeric, y)\n    mi_series = pd.Series(mi, index=X_numeric.columns)\n    sorted_mi_series = mi_series.sort_values(ascending=False)\n    \n    # Select top features\n    selected_features = sorted_mi_series.head(num_features).index.tolist()\n\n    # Plot mutual information scores\n    if plot:\n        plt.figure(figsize=(10, 6))\n        sorted_mi_series.plot(kind='bar')\n        plt.title('Mutual Information of Features with Survival')\n        plt.xlabel('Features')\n        plt.ylabel('Mutual Information Score')\n        plt.show()\n    \n    # Evaluate model performance with different numbers of top features\n    scores = []\n    num_features_range = range(1, 15)\n    for num in num_features_range:\n        selected_features = sorted_mi_series.index[:num]\n        X_selected = X_numeric[selected_features]\n        dt = DecisionTreeClassifier(random_state=42)\n        cv_scores = cross_val_score(dt, X_selected, y, cv=5, scoring='accuracy')\n        scores.append(cv_scores.mean())\n    \n    # Plot the model performance\n    plt.figure(figsize=(10, 6))\n    plt.plot(num_features_range, scores, marker='o')\n    plt.title('Model Performance vs. Number of Features')\n    plt.xlabel('Number of Features')\n    plt.ylabel('Accuracy')\n    plt.grid(True)\n    plt.show()\n\n    # Find the optimal number of features\n    optimal_num_features = num_features_range[scores.index(max(scores))]\n    print(f\"Optimal number of features: {optimal_num_features}\")\n    return sorted_mi_series.index[:optimal_num_features].tolist()\n\n\ndef train_decision_tree(X, y):\n    param_grid = { \n    'max_depth': [5, 10, 15, 20, 25],\n    'min_samples_split': [2, 4],\n    'min_samples_leaf': [10, 20, 30],\n    'ccp_alpha': [0.0, 0.001, 0.01],  # Added regularization parameter\n    'criterion': ['gini', 'entropy'],\n#     'splitter': ['best'],\n#     'max_depth': [3, 5, 10, 15, 20, None],\n#     'min_samples_split': [2, 3, 4, 5, 10],\n#     'min_samples_leaf': [1, 2, 5, 10, 20],\n#     'max_leaf_nodes': [None, 10, 20, 30, 40, 50],\n#     'max_features': ['sqrt', 'log2', None],\n#     'min_weight_fraction_leaf': [0.0, 0.05, 0.1],\n#     'ccp_alpha': [0.0, 0.01] \n    }\n    grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')\n    grid_search.fit(X, y)\n    \n    print(f\"Best parameters: {grid_search.best_params_}\")\n    print(f\"Best accuracy: {grid_search.best_score_}\")\n    \n    return grid_search.best_estimator_\n\n# Apply SMOTE + TOMEK\n\ndef apply_smote_tomek(X, y, sampling_strategy=0.75):\n    smote_tomek = SMOTETomek(sampling_strategy=sampling_strategy, random_state=42)\n    X_res, y_res = smote_tomek.fit_resample(X, y)\n    return X_res, y_res\n\n\n# Function to evaluate the model using confusion matrix and classification report\ndef evaluate_model(model, X, y):\n    y_pred = model.predict(X)\n    cm = confusion_matrix(y, y_pred)\n    cr = classification_report(y, y_pred)\n    accuracy = accuracy_score(y, y_pred)\n    fpr, tpr, _ = roc_curve(y, model.predict_proba(X)[:, 1])\n    roc_auc = auc(fpr, tpr)\n    \n    print(f\"Confusion Matrix:\\n{cm}\")\n    print(f\"Classification Report:\\n{cr}\")\n    print(f\"Accuracy: {accuracy}\")\n    \n    plt.figure(figsize=(10, 6))\n    plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f})')\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend(loc='lower right')\n    plt.show()\n\n    \ndef predict_and_prepare_submission(model, X_test, test, filename='submission_dt.csv'):\n    predictions = model.predict(X_test)\n    submission = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': predictions})\n    submission['Survived'] = submission['Survived'].map({1: 'T', 0: 'F'})\n    submission.to_csv(filename, index=False)\n    print(f\"Submission file '{filename}' created.\")\n    \n\n# Separate features and target variable from training data\nX_train = train_encoded.drop(columns=['Survived', 'Survived_binary'])\ny_train = pd.DataFrame(train_encoded['Survived_binary'], columns=['Survived_binary'])\n\n\n# Select best features for survival prediction using MI\nselected_survival_features = select_survival_features(X_train, y_train, num_features=5)\nX_train_selected = X_train[selected_survival_features]\n\n# Apply SMOTE to handle class imbalance + TOMEK to toss noisy data\nX_train_resampled, y_train_resampled = apply_smote_tomek(X_train_selected, y_train)\n# X_train_resampled, y_train_resampled = X_train_selected, y_train\n\n\n# Train and tune the Decision Tree classifier\nbest_dt = train_decision_tree(X_train_resampled, y_train_resampled)\n\n\n# Prepare test data for prediction\nX_test = test_encoded[selected_survival_features]\n\n# Predict and prepare the submission file\npredict_and_prepare_submission(best_dt, X_test, test)\n\n\n\n# Visualize the class distribution\nvisualize_data_analysis(y_train_resampled, \"Resampled Train Data\", [5], ['Survived_binary'])\nX_train_resampled.info()","metadata":{"execution":{"iopub.status.busy":"2024-07-15T18:30:01.028656Z","iopub.execute_input":"2024-07-15T18:30:01.029045Z","iopub.status.idle":"2024-07-15T18:30:15.152052Z","shell.execute_reply.started":"2024-07-15T18:30:01.029011Z","shell.execute_reply":"2024-07-15T18:30:15.150803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 10. K-fold cross validation and learning curve","metadata":{}},{"cell_type":"code","source":"def kfold_cross_validation(model, X, y, k_values=range(4,9)):\n    best_k = None\n    best_score = -np.inf\n    best_cross_val_scores = None\n\n    for k in k_values:\n        kf = KFold(n_splits=k, shuffle=True, random_state=42)\n        cross_val_scores = cross_val_score(model, X, y, cv=kf, scoring='accuracy')\n        mean_score = np.mean(cross_val_scores).round(2)\n        std_score = np.std(cross_val_scores).round(2)\n\n        print(f\"Cross-validation scores for k={k}: {cross_val_scores}\\n\")\n        print(f\"Mean cross-validation score for k={k}: {mean_score}\\n\")\n        print(f\"Standard deviation of cross-validation scores for k={k}: {std_score}\")\n\n\n        if mean_score > best_score:\n            best_score = mean_score\n            best_k = k\n            best_std = std_score\n            best_cross_val_scores = cross_val_scores\n\n    print(f\"Best k: {best_k} \\n\")\n    print(f\"Best mean cross-validation score: {best_score}\\n\")\n    print(f\"Standard deviation of the best cross-validation scores: {best_std} \\n\\n\\n\")\n\n    return best_k, best_cross_val_scores\n\n\n\ndef plot_learning_curve(estimator, title, X, y, cv=None, n_jobs=None, train_sizes=np.linspace(0.1, 1.0, 10)):\n    plt.figure(figsize=(10, 6))\n    plt.title(title)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n\n    train_sizes, train_scores, valid_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring='accuracy')\n\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    valid_scores_mean = np.mean(valid_scores, axis=1)\n    valid_scores_std = np.std(valid_scores, axis=1)\n\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n    plt.fill_between(train_sizes, valid_scores_mean - valid_scores_std,\n                     valid_scores_mean + valid_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n    plt.plot(train_sizes, valid_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\n# Evaluate the model by ROC and conf matrix\nevaluate_model(best_dt, X_train_resampled, y_train_resampled)\n\n# Plot the learning curve\nbest_k,_= kfold_cross_validation( best_dt, X_train_resampled, y_train_resampled)\nplot_learning_curve(best_dt, f\"Learning Curve (Decision Tree) with k={best_k}\", X_train_resampled, y_train_resampled, cv=best_k)\nplt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-15T18:30:15.155837Z","iopub.execute_input":"2024-07-15T18:30:15.156377Z","iopub.status.idle":"2024-07-15T18:30:17.692099Z","shell.execute_reply.started":"2024-07-15T18:30:15.156334Z","shell.execute_reply":"2024-07-15T18:30:17.690812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 11. Plot tree","metadata":{}},{"cell_type":"code","source":"# Plot tree\n\nfrom sklearn.tree import  plot_tree\n# Function to plot the decision tree\ndef plot_decision_tree(model, feature_names):\n    plt.figure(figsize=(20, 10))\n    plot_tree(model, feature_names=feature_names, filled=True, class_names=['Not Survived', 'Survived'], rounded=True)\n    plt.show()\n    \nplot_decision_tree(best_dt, selected_survival_features)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T18:30:17.694228Z","iopub.execute_input":"2024-07-15T18:30:17.694736Z","iopub.status.idle":"2024-07-15T18:30:19.267553Z","shell.execute_reply.started":"2024-07-15T18:30:17.694693Z","shell.execute_reply":"2024-07-15T18:30:19.266205Z"},"trusted":true},"execution_count":null,"outputs":[]}]}